{"cells":[{"cell_type":"markdown","metadata":{"id":"GJz5HobCMdZo"},"source":["# <font color='#eb3483'> HTTP Requests \\& APIs </font>\n","\n","In this notebook we are going to learn how to get information from a website. The fancy way of saying that is to do **HTTP requests**. HTTP requests have methods that let you **GET** and POST data. We'll focus on the GET method which is used for information retrieval.\n","\n","Some extra reading material: https://realpython.com/python-requests/\n","\n","The most common library used to do HTTP requests in python is the `requests` library (see [documentation](https://requests.readthedocs.io/en/master/))."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJ2ankhtMdZz"},"outputs":[],"source":["import requests"]},{"cell_type":"markdown","metadata":{"id":"1IeoU-K6MdZ4"},"source":["## <font color='#eb3483'> GET Requests </font>\n","\n","To retrieve a resource from an online service, we us the HTTP **GET** method, which can be called with `requests.get(url)`. For example, if we want to get the main page at [Wikipedia](https://wikipedia.com/) we can do it like this:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mrkjljSZMdZ6"},"outputs":[],"source":["response = requests.get(\"https://wikipedia.com/\")"]},{"cell_type":"markdown","metadata":{"id":"_AVU9wG2MdZ7"},"source":["We can check the status of the request to verify that the request was successful."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F5_ivee4MdZ9"},"outputs":[],"source":["response.status_code"]},{"cell_type":"markdown","metadata":{"id":"hzvld05qMdZ-"},"source":["In this case we get a **200** status code, meaning that our request was successful and the server returned the data we requested. You can check the meaning of standard response statuses [here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status). \n","\n","A shortcut for checking for success is:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Prfhdws3MdaA"},"outputs":[],"source":["response.ok"]},{"cell_type":"markdown","metadata":{"id":"WpHZbqJuMdaB"},"source":["We can see what data the server has returned with `response.content` (bytes) or `response.text` (string)."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"JB4Uw3n4MdaC"},"outputs":[],"source":["response.content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fz5j0NDkMdaD"},"outputs":[],"source":["type(response.content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GxnuV9OgMdaE"},"outputs":[],"source":["response.text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7O7Tt81GMdaE"},"outputs":[],"source":["type(response.text)"]},{"cell_type":"markdown","metadata":{"id":"y_-IkGEeMdaF"},"source":["We can now save the request content to an html file and open it locally."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Esgtr3vxMdaF"},"outputs":[],"source":["with open(\"wikipedia.html\", \"wb\") as fname:\n","    fname.write(response.content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJtTgY_oMdaG"},"outputs":[],"source":["import webbrowser\n","webbrowser.open(\"./wikipedia.html\")"]},{"cell_type":"markdown","metadata":{"id":"aTOva72pMdaH"},"source":["The Wikipedia page we got via `requests` has most of the relevant page content, but some of the style elements are missing. This is because `requests.get()` only gets the html content, it doesn' load the css or other resource that the page needs."]},{"cell_type":"markdown","metadata":{"id":"53F_tWTiMdaI"},"source":["## <font color='#eb3483'>GETing useful responses</font>\n","### Reddit\n","\n","Let's try to get the top 5 posts from the `/r/datascience` subreddit."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DbrHqe4HMdaI"},"outputs":[],"source":["response = requests.get(\"http://www.reddit.com/r/datascience\")\n","response.ok"]},{"cell_type":"markdown","metadata":{"id":"xeWuYhATMdaJ"},"source":["Oh no. It looks like our request did not work. Let's check the status code."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6AuGZroMdaJ"},"outputs":[],"source":["response.status_code"]},{"cell_type":"markdown","metadata":{"id":"cm8UmJ7eMdaJ"},"source":["We know that status codes starting with *2xx* indicate success. Statuses starting with *5xx* indicate server errors, meaning that the server from which we requested data experienced a problem. To get more details, we can check the attribute `reason` in the response object."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z9riSuUpMdaK"},"outputs":[],"source":["response.reason"]},{"cell_type":"markdown","metadata":{"id":"uVdES2tzMdaO"},"source":["**Bad Gateway?** That's weird. We can stil open reddit on our browser."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P2hTyqr3MdaO"},"outputs":[],"source":["webbrowser.open(\"http://www.reddit.com/r/datascience\")"]},{"cell_type":"markdown","metadata":{"id":"Zg2GzoG3MdaP"},"source":["When we do an http request, we send **Request Headers** alongside the request method (GET) and url. The headers provide metadata about the request and the requester. Let's check our requests headers:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pscrxBZuMdaP"},"outputs":[],"source":["response.request.headers"]},{"cell_type":"markdown","metadata":{"id":"IPY8WWFuMdaQ"},"source":["The **User-Agent** header identifies the requester. Our user agent is set to `python-requests/2.22.0`, which is pretty generic for a python application. This may make Reddit suspicious of our request, and our access is denied.\n","\n","We can fix this by providing our own user agent with the `headers` argument."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vVG3zHlnMdaQ"},"outputs":[],"source":["response = requests.get(\n","    \"http://www.reddit.com/r/datascience\", \n","    headers={\"User-Agent\":\"My User Agent 1.0\"})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uy_JFls1MdaR"},"outputs":[],"source":["response.ok"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dyR5VTZJMdaR"},"outputs":[],"source":["response.content"]},{"cell_type":"markdown","metadata":{"id":"F9ZLdRTyMdaS"},"source":["Sweet, it worked! But the content is a pain to parse. Thankfully Reddit has a json API. Data **API**s (short for Application Programming Interface) are frequently made available by data providers to manage data access and faclitate common requests. Let's take advantage of it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHFyEuNVMdaT"},"outputs":[],"source":["response = requests.get(\"http://www.reddit.com/r/datascience/.json?limit=5\", \n","                        headers={\"User-Agent\":\"My User Agent 1.0\"})\n","\n","response.headers"]},{"cell_type":"markdown","metadata":{"id":"-giPywDeMdaU"},"source":["The API returns the response as json, which has the same format as the dictionary data type. We can use the `requests.json()` method to parse it: "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JxMv1a6GMdaV"},"outputs":[],"source":["data = response.json()"]},{"cell_type":"markdown","metadata":{"id":"Fe-VR-boMdaW"},"source":["And make the response look pretty:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iBrI2evpMdaX"},"outputs":[],"source":["from pprint import pprint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"60654U6mMdaY"},"outputs":[],"source":["pprint(data)"]},{"cell_type":"markdown","metadata":{"id":"Doevj8ByMdaY"},"source":["The first key in the dictionary is `data` and one of the level-one nested keys is `children` with the actual posts. We can get the title for the second post with:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fECTNvB5MdaY"},"outputs":[],"source":["data['data']['children'][1]['data']['title']"]},{"cell_type":"markdown","metadata":{"id":"onJHtCq-MdaZ"},"source":["If we want to get all the posts' titles we just have to iterate:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Llh8cqMkMdaZ"},"outputs":[],"source":["titles = []\n","for post in data['data']['children']:\n","    titles.append(post['data']['title'])\n","    \n","titles"]},{"cell_type":"markdown","metadata":{"id":"pi14lHEcMdaa"},"source":["This gets a bit complicated. An alternative is to use [`glom`](https://github.com/mahmoud/glom) which can iterate dictionaries much faster.  \n","`conda install -c conda-forge glom`  \n","**NOTE:** `glom` is a fairly new library so it might be unstable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6SsN3UzuMdaa"},"outputs":[],"source":["from glom import glom\n","glom(data, ('data.children', ['data.title']))"]},{"cell_type":"markdown","metadata":{"id":"QQ1vnLSAMdaa"},"source":["## <font color='#eb3483'>Tweeting up a Storm</font>"]},{"cell_type":"markdown","metadata":{"id":"EKaZjlLiMdaa"},"source":["Many apps manage access to their data much more tightly with APIs. For example, while data from the Twitter website can be retrieved with an HTTP request, it won't give you much useful information. Instead, Twitter has created an API that allows developers to access much richer subsets of their data, based on user permissions. \n","\n","You can access the [Twitter API](https://developer.twitter.com/en/docs/basics/getting-started) directly, or use the [tweepy](http://docs.tweepy.org/en/latest/getting_started.html) python library, which is more convenient. We will use tweepy.\n","\n","To access the data, you need to be a Twitter user and follow an authentication process for a developer account.\n","\n","1. Create a twitter account\n","2. Create a twitter [app](https://developer.twitter.com/en/apps) by following the link and selecting the **create an app** button\n","3. Select the **Student** option from the Academic column and fill in the necessary details in the forms that follow. For security reasons Twitter asks several questions on how you intend to use the data. You need to answer these and can elaborate that you are learning to use APIs as part of a Data Science course.\n","4. Twitter will now review your request, which may take a while.\n","\n","Once your developer account has been approved, you can set up access fpr the Twitter API. You will need to provide your credentials to the API before you can retrieve data with python. It is good practice to read in sensitive information like credentials from a separate file. We will use a `yaml` file to store our Twitter credentials, as yaml is human readable and commonly used for providing credentials."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-LIY1k8WMdab"},"outputs":[],"source":["import tweepy\n","import yaml"]},{"cell_type":"markdown","metadata":{"id":"qBxgTUFTMdab"},"source":["Create a `credentials.yml` file in your working directory. The file must contain your *consumer_key*, *consumer_secret*, *access_token* and *access_token_secret*. You can retrieve them from within the Twitter app (after it has been approved). The credentials file must be formatted as follows:\n","\n","```\n","consumer_key: my_key\n","consumer_secret: my_secret\n","access_token: my_token\n","access_token_secret: my_token_secret\n","```\n","\n","Next, load the credentials file so that you can pass your credentials to the Twitter API."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OowW-B2LMdab"},"outputs":[],"source":["with open('credentials.yml') as f:\n","    credentials = yaml.load(f, Loader=yaml.FullLoader)\n","    \n","credentials"]},{"cell_type":"markdown","metadata":{"id":"0kh-GHjEMdab"},"source":["When authenticating the API, you may need to grant it access by accepting standard OAuth pop-ups (more information on authentication [here](http://docs.tweepy.org/en/latest/auth_tutorial.html))."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0K88_qYMdac"},"outputs":[],"source":["auth = tweepy.OAuthHandler(credentials['consumer_key'], credentials['consumer_secret'])\n","auth.set_access_token(credentials['access_token'], credentials['access_token_secret'])\n","\n","api = tweepy.API(auth)"]},{"cell_type":"markdown","metadata":{"id":"VzW9GyfLMdac"},"source":["And finally you are ready to get some tweets. The `user_timeline()` method returns the 20 most recent statuses posted from the authenticating user or another user specified via the id parameter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"csrdCP5hMdac"},"outputs":[],"source":["public_tweets = api.user_timeline()\n","for tweet in public_tweets:\n","    print(tweet.text)"]},{"cell_type":"markdown","metadata":{"id":"p5vpQiG6Mdac"},"source":["To do more fun things, you can browse through the [tweepy API reference guide](http://docs.tweepy.org/en/latest/api.html)."]},{"cell_type":"markdown","metadata":{"id":"xCp1tEZeMdad"},"source":["### Exercise\n","\n","Post a tweet to share you experience of the first two days of the Data Science course. Here is some inspiration:\n","\n","```\n","\"Just started learning #datascience with @iXperienceCT. Tweeting from my jupyter notebook. Love my TA @wiebketous. Wish I was in Cape Town though.\"\n","\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ue4b743DMdad"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"},"colab":{"name":"W1D2 HTTP Requests _ APIs.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}
