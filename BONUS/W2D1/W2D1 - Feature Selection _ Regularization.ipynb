{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#eb3483'> Feature Selection and Regularization in Linear Regression</font>\n",
    "\n",
    "Before working through this notebook, you should read Chapter 6, Sections 6.1 and 6.2 of [Introduction to Statistical Learning](https://static1.squarespace.com/static/5ff2adbe3fe4fe33db902812/t/6062a083acbfe82c7195b27d/1617076404560/ISLR%2BSeventh%2BPrinting.pdf). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'> Import and Explore the Data </font>\n",
    "\n",
    "For this notebook, we'll be using the Hitters data that is provided as a csv file in the `data` folder.\n",
    "\n",
    "We wish to predict a baseball player's salary on the basis of various statistics associated with performance in the previous year.\n",
    "\n",
    "Let's import the data and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Hitters.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some `Salary` values seem to be `NaN`. Let's check for missing values across all the variables and remove the rows with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rows before = ', df.shape[0])\n",
    "df.dropna(inplace=True)\n",
    "print('Rows after = ', df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also deal with those categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.League.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Division.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.NewLeague.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['League'] = 1*(df.League=='A')\n",
    "df['Division'] = 1*(df.Division=='W')\n",
    "df['NewLeague'] = 1*(df.NewLeague=='A')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's standardize each predictor; that is, we subtract the predictor's mean and divide by the predictor's standard deviation. This way, each column will be centred around zero and have the same scale. This is required for the regularization techniques that we will look at below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df[df.columns[df.columns!='Salary']])\n",
    "X = pd.DataFrame(X, columns = df.columns[df.columns!='Salary'])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin our analyses by fitting a standard multiple regression model to predict `Salary`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "olsmod = LinearRegression()\n",
    "olsmod.fit(X=X, y=df.Salary)\n",
    "pd.Series(olsmod.coef_, index=X.columns).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the features all have the same scale, we can tell from the coefficients which ones are most influential. Higher values of `CRuns`, `Hits` `CRBI` and `Walks` and lower values of `CAtBat`, `AtBat` and `CWalks` are strongly associated with a higher `Salary`.\n",
    "\n",
    "Can we drop some of the intermediate features, those that don't seem to be strongly associated with `Salary`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'> Recursive Feature Elimination </font>\n",
    "\n",
    "In Section 6.1 of [Intro to Stat Learning]((https://static1.squarespace.com/static/5ff2adbe3fe4fe33db902812/t/6062a083acbfe82c7195b27d/1617076404560/ISLR%2BSeventh%2BPrinting.pdf)), the authors describe stepwise and best subset selection. These are not implemented in scikit-learn. However, scikit-learn does have a procedure called [Recursive Feature Elimination (RFE)](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html) that does feature selection. You can read more about it through the link above. In a nutshell, the procedure begins by fitting a model with all the features and then recursively removes them from the model based on how much they contribute to model performance (the worst contributors are eliminated first). This is very similar to the *backward stepwise* procedure described in Section 6.1. \n",
    "\n",
    "RFE is available via the `RFE` function from the `feature_selection` submodule in `sklearn`. This function requires that you specify the number of features that you would like in your model, say, 5 and it then determines which 5 features are the best. Alternatively, you can use the `RFECV` function to determine the optimal *number* of features as well. Note that this function can be applied to any `sklearn` estimator, though we will only consider linear regression below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "?RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFECV(estimator=olsmod, cv=5, scoring=\"neg_mean_squared_error\")\n",
    "rfe.fit(X=X, y=df.Salary)\n",
    "print('Selected', rfe.n_features_, 'features:')\n",
    "X.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these are the features with the largest coefficients (in absolute value) in our full model above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'> Ridge Regression </font>\n",
    "\n",
    "Section 6.2 of [Intro to Stat Learning]((https://static1.squarespace.com/static/5ff2adbe3fe4fe33db902812/t/6062a083acbfe82c7195b27d/1617076404560/ISLR%2BSeventh%2BPrinting.pdf)) describes ridge regression. In short, ridge regression uses a different loss function to OLS regression. Recall that OLS regression finds the coefficients that minimise the residual sum of squares (RSS) loss. Ridge regression adds a penalty term to this to prevent coefficients from getting too big. The extent to which this penalty plays a role is determined by a hyperparameter $\\lambda$. When $\\lambda = 0$, we get OLS regression (no penalty). Larger values of $\\lambda$ lead to more *regularization* - the coefficients get shrunk toward zero. You can see this as making the model simpler. Thus increasing $\\lambda$ increases bias and reduces variance (the bias-variance trade-off!). As always, to choose the best compromise between bias and variance (and hence the best $\\lambda$), we use cross validation. \n",
    "\n",
    "Note that $\\lambda$ is called \"alpha\" in the `sklearn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "?RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeCV(alphas=[0.1, 1.0, 10.0], scoring=\"neg_mean_squared_error\", cv=5)\n",
    "ridge.fit(X=X, y=df.Salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.alpha_ # this is the best value of alpha based on CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'ols':olsmod.coef_, 'ridge':ridge.coef_}, index=X.columns)\n",
    "# notice how the ridge regression coefficients are often (not always) pulled closer to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'> Lasso Regression </font>\n",
    "\n",
    "Finally, Section 6.2 of [Intro to Stat Learning]((https://static1.squarespace.com/static/5ff2adbe3fe4fe33db902812/t/6062a083acbfe82c7195b27d/1617076404560/ISLR%2BSeventh%2BPrinting.pdf)) also describes lasso regression or \"the lasso\". Conceptually, the lasso is very similar to ridge regression - it adds a penality to the residual sum of squares to rein in the regression coefficients. It also has a hyperparameter $\\lambda$ that controls the influence of the penalty (the bias-variance trade-off). The main difference is that the mathematical form of the lasoo penalty allows the lasso to set some coefficients to exactly zero i.e. it can perform feature elimination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "?LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(n_alphas=100, cv=5)\n",
    "lasso.fit(X=X, y=df.Salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.alpha_ # this is best value of lambda based on CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(lasso.coef_, index=X.columns) \n",
    "# notice how many of these have been set to 0 (dropped from the mode)\n",
    "# this is a special property of the lasso penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "ridge_best = Ridge(alpha=ridge.alpha_)\n",
    "lasso_best = Lasso(alpha=lasso.alpha_)\n",
    "\n",
    "cv_ols = cross_val_score(estimator=olsmod, X=X, y=df.Salary, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "cv_rfe = cross_val_score(estimator=olsmod, X=X[X.columns[rfe.support_]], y=df.Salary, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "cv_ridge = cross_val_score(estimator=ridge_best, X=X, y=df.Salary, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "cv_lasso = cross_val_score(estimator=lasso_best, X=X, y=df.Salary, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "\n",
    "print('OLS MSE:', -cv_ols.mean())\n",
    "print('RFE MSE:', -cv_rfe.mean())\n",
    "print('Ridge MSE:', -cv_ridge.mean())\n",
    "print('Lasso MSE:', -cv_lasso.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, it looks like RFE is the best procedure!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
