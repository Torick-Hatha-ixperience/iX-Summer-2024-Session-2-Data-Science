{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#eb3483'> PyCaret </font>\n",
    "\n",
    "Last week we learned a great toolkit of machine learning models and how to build, train, and test them in isolation but what if we don't want to try out each model independently? PyCaret (the python ~~rip-off~~ version of caret - R's premiere machine learning interface) provides an awesome interface to quickly test a gaggle of machine learning algorithms with only a handful of code. In this notebook we'll check it out and show you some helpful functionality.\n",
    "\n",
    "Start by installing the package using pip if you haven't already:\n",
    "`pip install pycaret`\n",
    "\n",
    "### <font color='#eb3483'> Credit Dataset </font>\n",
    "For this tutorial we'll be using the credit dataset from [UCI](https://archive.ics.uci.edu/ml/index.php) (a really great resource for machine learning datasets). PyCaret, like sklearn and seaborn, has some great in-built functionality for getting data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pycaret.datasets import get_data\n",
    "credit_raw = get_data('credit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has 23 features about a loan applicant, and whether or not they defaulted on the loan (default variable). Our goal is going to be to predict whether or not someone will default. We're going to hold a little bit of the data back for testing our models later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = credit_raw.sample(frac=0.95, random_state=123)\n",
    "test_data = credit_raw.drop(credit.index).reset_index(drop=True)\n",
    "credit.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> PyCaret Environment </font>\n",
    "To start working with our data in pycaret we need to import a module based on the type of problem we're solving (in this case classification, but check online for other options), and set-up a pycaret environment using the setup function. This will check the datatype of our columns and do some important pre-processing steps. PyCaret is built to be used in jupyter or google colab so it'll have some interactive steps for you to check what's going on and confirm they've interpreted the data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the classification module \n",
    "from pycaret import classification\n",
    "# setup the environment \n",
    "#classification_setup = classification.setup(data= data_classification, target='Personal Loan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "exp_clf101 = setup(data = credit, target = 'default', session_id=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew there's a lot going on here! It's not super important that we understand every step for this tutorial, but read through the list and you'll see that pycaret can automatically take care of a lot of things we've talked about the past couple weeks including imputing missing data, normalizing and breaks our data into a train and test set. We haven't checked most of the options, but these are awesome steps to keep in mind when you want to use PyCaret to customize your data cleaning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> Comparing Models </font>\n",
    "Where PyCaret shines is quickly comparing a multitude of machine learning models. The compare models function runs through all the models in the module you're using (i.e. classification like us) and outputs some high level metrics to help you see what types of models seem to work best on your data. This'll take awhile (as it should - we're training a bunch of models)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that PyCaret's even saved us the trouble of trying to look at a table and see what model's best - it's sorted it by accuracy (and if you change the sort parameter you can get it to sort by other metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> Training A Model </font>\n",
    "Compare models is an amazing tool that gives you an overview of which algorithms perform best, but it doesn't actually return us a trained model. It also isn't able to do hyper-parameter tuning for each model so once you have an idea of what models are worth trying it's important to actually train one on it's own. Luckily PyCaret has a convenient interface for training models.\n",
    "\n",
    "Based on the AUC metric it looks like extreme gradient boosting is the way to go for this dataset. Let's train a model using it. We can see what the abbreviated string for each model type is [here](https://pycaret.org/create-model/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All we have to do to train a model is use create_model and specify we want XGBoost (aka Extreme Gradient Boosting)\n",
    "xgb = create_model('xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even tune hyperparameters for a model using the `tune_model` function. It works by testing a simple grid of possible hyperparameter values and chosing the one with thebest accuracy, but it provides a lot of flexibility for specifying exactly how you want to tune it (check out the help docs for all the options!). Let's train a tuned decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_dt = tune_model('dt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow notice how much higher our decision tree performance is after tuning versus when we just used compare model! A nice reminder that hyper parameter is super important for getting rockstar machine learning results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> Ensemble Methods </font>\n",
    "So far we've talked about training and testing models in isolation, but often times the best solution is to use a few different algorithms and combine their predictions, a process called *ensembling*. This is a massive topic so we won't dive deep into it in this notebook, but feel free to check-out some more information [here](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/?utm_source=blog&utm_medium=pycaret-machine-learning-model-seconds). \n",
    "\n",
    "PyCaret let's us combine models into an ensemble using a few different methods, but let's use the blending method and give it the two classifier's we've already trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blender = blend_models(estimator_list=[xgb, tuned_dt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately it doesn't look like ensembling has led to a massive increase in performance on this problem with the models we included, but in general it's a great tool to try out when you're trying to eek out a few extra percentage points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> Plotting Performance </font>\n",
    "We all know data visualization is important - and man oh man does PyCaret have some great tools for visualizing our model's performance (looks like a greatest hits of our class from this week). I won't explain every plot (there are over 15 options) - but let's take a peak at a couple familiar plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(tuned_dt, plot = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_model(tuned_dt, plot = 'confusion_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(tuned_dt, plot='feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of plotting each curve independently, PyCaret even provides an interface for selecting what we want to see (we're barely even coding anymore!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(tuned_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> Prediction </font>\n",
    "Obviously if we're training a machine learning model, we'll eventually want to use it to make some predictions! To make predictions on unseen data, you can use the `predict_model` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_model(tuned_dt, data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output is the data with two new columns - label and score representing our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> Next Steps </font>\n",
    "PyCaret has a lot of functionality, and this notebook just scratches the surface. For more information make sure to check out their website and the fantastic tutorials they have [here](https://pycaret.org/guide/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
