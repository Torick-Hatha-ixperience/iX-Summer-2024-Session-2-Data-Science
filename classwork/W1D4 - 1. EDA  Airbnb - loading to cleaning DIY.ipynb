{"cells":[{"cell_type":"markdown","metadata":{"id":"MLU9sOazG4KM"},"source":["# <font color='#eb3483'> Exploratory Data Analysis </font>\n"]},{"cell_type":"markdown","metadata":{"id":"pN5dCHHiG4KO"},"source":["## How to perform Exploratory Data Analysis?  \n","\n","This is one a question that everyone is keen on knowing the answer to. Well, the answer is it depends on the data set that you are working on. There is no one method or common methods in order to perform EDA. However, there are a few set steps that this generally includes:  \n","1. Loading and inspecting your data\n","1. Cleaning the data which includes:  \n","   2.1. dropping data points and columns we don't need.   \n","   2.2. checking data types and fixing if needed  \n","   2.3. removing duplicates  \n","   2.4. dealing with missing values  \n","   2.5. looking for outliers and deciding how to deal with these  \n","   2.6. reformatting columns if needed\n","\n","1. Some visual exploration to look at relationships between variables or interesting insights that jump out. Plotting in EDA consists of Histograms, Box plot, Scatter plot and many more.   \n","\n","1. How can you add, change or remove features to get more out of your data? (aka Feature engineering)"]},{"cell_type":"markdown","metadata":{"id":"eQalgxWtG4KO"},"source":["# <font color='#eb3483'> AirBnB Cape Town </font>\n"]},{"cell_type":"markdown","metadata":{"id":"9ksA0dYGG4KO"},"source":["\n","\n","### Background\n","\n","\n","http://insideairbnb.com/get-the-data.html\n","\n","\n","* What can we say about the prices of air bnbs in Cape Town.   \n","* Are there certain neighbourhoods that are more expensive.   \n","* Do properties with higher ratings charge more?  \n","* Does more rooms mean more money?\n","\n","This is a great place to start digging in to these questions or generating hypotheses, with data on the price, neighbourhood, layout and ratings per air bnb rental."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htQn5bG4G4KO"},"outputs":[],"source":["# Importing the required packages here\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import ast, json\n","\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"sj3STGpcG4KP"},"source":["## <font color='#eb3483'> 1. Loading and inspect your data </font>\n"]},{"cell_type":"code","source":["# import the data"],"metadata":{"id":"cWK3xszSKEIl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2bRiA5CgG4KP"},"source":["What other commands can we use to have a high level glance at our data frame?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r7C3e2I2G4KQ"},"outputs":[],"source":["#what are some interesting insights we can pull out from looking at the dataframe?"]},{"cell_type":"markdown","metadata":{"id":"C2qNuAE9G4KQ"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"Oy3x40cWG4KQ"},"source":["## <font color='#eb3483'> 2. Clean the data </font>\n","\n","Some (but potentially not all) steps to take\n","2.  Cleaning the data which includes:  \n","   2.1. dropping columns we don't need  \n","   2.2. removing duplicates  \n","   2.3. checking data types and fixing if needed  \n","   2.4. dealing with missing values  \n","   2.5. looking for outliers and deciding how to deal with these\n","\n"]},{"cell_type":"markdown","metadata":{"id":"k7H8tP3jG4KQ"},"source":["## <font color='#eb3483'> 2.1. Drop columns we dont need </font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T22xK5eZG4KQ"},"outputs":[],"source":["# removing uneeded columns - lets have a browse through see what we can probably remove\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0sXeSQnLG4KQ"},"outputs":[],"source":["#use unique command to check out whats in a column if needed\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-jWloCLiG4KQ"},"outputs":[],"source":["# First step is to clean the data and see which are the redundant or unnecessary cols\n","df = df.drop([\"summary\", \"name\",\"host_url\", \"host_id\",\"host_name\", \"square_feet\",\"host_about\", \"minimum_minimum_nights\", \"minimum_maximum_nights\"], axis=1)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WnWvEr_GG4KQ"},"outputs":[],"source":["# do we waant to rename any columns?\n","# lets rename neighbourhood_cleansed to neighbourhood\n","\n","df = df.rename(columns={\n","    \"neighbourhood_cleansed\":\"neighbourhood\",\n","})\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"89jb8stgG4KQ"},"source":["## <font color='#eb3483'> 2.2. Removing duplicates </font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKtEzMh2G4KQ"},"outputs":[],"source":["#Let's look for duplicate rows\n","\n","print(df.shape)\n","\n","# Rows containing duplicate data\n","duplicate_rows_df = df[df.duplicated()]\n","\n","print(duplicate_rows_df.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xl6vG6cwG4KR"},"outputs":[],"source":["# whoooops! not cool - let's remove them.\n","df = df.drop_duplicates(keep='first')\n","\n","print(df.shape)"]},{"cell_type":"markdown","metadata":{"id":"9LCAbRG6G4KR"},"source":["## <font color='#eb3483'> 2.3. Check data types </font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dOOmFJikG4KR"},"outputs":[],"source":["#lets have a quixk look at our data types\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_W0SPnLkG4KR"},"outputs":[],"source":["#Any issues here?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgjdoXUfG4KR"},"outputs":[],"source":["# How about changing the host_since to DateTime column.\n","\n","df.host_since = pd.to_datetime(df[\"host_since\"])\n","print(df.dtypes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qdPj2GwfG4KR"},"outputs":[],"source":["#lets change host ID to an object. isnt really a number - its a category\n","# cant remember how - google is your friend."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qj6yfYVtG4KR"},"outputs":[],"source":["df[\"id\"]=df[\"id\"].astype(object)\n","print(df.dtypes)"]},{"cell_type":"markdown","metadata":{"id":"uhsQGUWlG4KR"},"source":["## <font color='#eb3483'> 2.3. Missing values </font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xABbVEzKG4KR"},"outputs":[],"source":["print(df.isnull().sum())"]},{"cell_type":"markdown","metadata":{"id":"-DQpJ8WJG4KR"},"source":["### <font color='#eb3483'>Missigno </font>\n","This package let's us view how our missing data is spread out across rows and columns in a super convenient visual format (package found her: https://github.com/ResidentMario/missingno)\n","\n","You can install this package by using `conda install -c conda-forge missingno`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q8GiBm5VG4KR"},"outputs":[],"source":["import missingno as msno\n","\n","msno.matrix(df);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5MC-gI_-G4KV"},"outputs":[],"source":["# what do we see here.\n","\n","# square feet - doesnt look like there is much useable info here - so lets drop that one.\n","# cleaning fee - also quite a few missing values. - you can decide on this one. or maybe we impute the average cleaning fee for missing numbers.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7xWUuW5_G4KV"},"source":["What do we do about missing values ...\n","- Remove those records with missing values  \n","`df = df.dropna()`   \n","or  \n","`df = df.dropna(subset=[\"column2\", \"column5\", \"this_column\", \"that_column\"])`  \n","\n","\n","- Replace the null values with a particular value, for example 0 or \"missing\". It is a simple technique but adds noise (because it assumes the null values are one specific case).  \n","`df[\"column1\"] = df.column1.fillna(\"missing\")`  \n","`df[\"this_column\"] = vehicles.this_columns.fillna(0)`  \n","\n","\n","- Data Imputation: We can replace the missing values with a particular value, but use some criteria to choose that value. Common imputation practices are imputing with the mean, mode or median."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrXPc0whG4KV"},"outputs":[],"source":["# impute missing values for cleaning fee - do distributions and impute with mean.\n","review_scores_rating_mean = df.review_scores_rating.mean()\n","\n","df.review_scores_rating = df.review_scores_rating.fillna(review_scores_rating_mean)\n"]},{"cell_type":"markdown","metadata":{"id":"SWl5eJRZG4KV"},"source":["There are other techniques to deal with missing values:\n","\n","- Use a predictive model to predict the missing values.\n","\n","- More sophisticated methods: [MICE](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/) is a method that deals with missing values, and in this [repository](https://github.com/hammerlab/fancyimpute) there are more methods\n","\n","[Here](https://gallery.cortanaintelligence.com/Experiment/Methods-for-handling-missing-values-1) there are more strategies"]},{"cell_type":"markdown","metadata":{"id":"upZxSKw4G4KV"},"source":["## <font color='#eb3483'> 2.4. Looking for and dealing with outliers </font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yiZiFG2CG4KV"},"outputs":[],"source":["df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVs2j0-BG4KV"},"outputs":[],"source":["# lets check out accommodates\n","df.accommodates.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3qIJBS_G4KV"},"outputs":[],"source":["#make a histogram for accommodates\n","sns.displot(df.accommodates, bins=20, rug=True);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qL7JnHcrG4KV"},"outputs":[],"source":["#make a boxplot for accommodates\n","sns.boxplot(df.accommodates)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qSxTCajpG4KV"},"outputs":[],"source":["# lets check out the unique values (always useful)\n","df.accommodates.unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqse7mvPG4KV"},"outputs":[],"source":["# hmmmmm, 40 looks quite high. lets check this out ... is this a true outlier?\n","df[df.accommodates > 20]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmcMQIoiG4KW"},"outputs":[],"source":["#Check out price in the same way."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trnIJGk7G4KW"},"outputs":[],"source":["# what if we HAD LOTS OF VARIABLES and wanted to look at all the numerical values at once.\n","numerical = [\n","  'accommodates', 'bathrooms', 'bedrooms', 'beds', 'price', 'number_of_reviews', 'review_scores_rating'\n","]\n","\n","#could also pull out columns based on dtype\n","#numerical = df.select_dtypes(include=np.number) # different way of doing the above.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"akA-0tU4G4KW"},"outputs":[],"source":["df[numerical].hist(bins=15, figsize=(15, 8), layout=(3, 4));"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ODQXZo7YG4KW"},"outputs":[],"source":["# Another fun way to view this is using widgets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JOm1b-DMG4KW"},"outputs":[],"source":["from ipywidgets import interact, fixed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i6tV2ZHEG4KW"},"outputs":[],"source":["#Create our plotting function\n","def numerical_variable_distribution(df, col):\n","    df[col].plot.hist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2Ffd2CSG4KW"},"outputs":[],"source":["numerical_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n","interact(numerical_variable_distribution,\n","         col=numerical_columns, df=fixed(df));"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ywYpgWjUG4KX"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"RmthP1LpG4KX"},"source":["EXERCISE: In break out rooms investigate cleaning fee, number of reviews, bedrooms, batherooms, beds and review scores rating, for 10-15 mins come back and report what you found."]},{"cell_type":"markdown","metadata":{"id":"uXXQSjPwG4KX"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"M4sAC-rkG4KX"},"source":["In general, extreme values are those values that are different than the variable distribution, and estimating summary statistics for a column with outliers yields unreliable results.\n","\n","One common practice is to consider outliers those values with a z score higher than 3 (that means, they are 3 standard deviations   bigger or smaller than the mean.\n","\n","z score is defined as:\n","\n","$$z(x)= \\frac{x-\\mu}{\\sigma}$$\n","\n","So you can always double check whether values fall above this for potential extreme outliers."]},{"cell_type":"markdown","metadata":{"id":"7qimF7N4G4KX"},"source":["### How do we assess categorical variables ?"]},{"cell_type":"markdown","metadata":{"id":"-BOoPCv9G4KX"},"source":["It is good practice to look at the categorical variables to get an idea of the cardinality - and how useful they might be in groupings or as predictive varaibles.\n","\n","\n","High cardinality =  variables with few repeated values (ie all different)\n","Low cardinality = many repeated values (ie almost all one type)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mjIy9-spG4KX"},"outputs":[],"source":["df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"DzJ1-ecOG4KX"},"outputs":[],"source":["sns.countplot(y = df['room_type']);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7lIgMAmsG4KX"},"outputs":[],"source":["#Or we can view this as a ratio\n","df.room_type.value_counts(normalize=True).plot.barh(); # as a ratio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nE_nDlRPG4KX"},"outputs":[],"source":["def categorical_variable_distribution(col):\n","    df[col].value_counts(ascending=True,normalize=True).tail(20).plot.barh() # only pulls out 20 results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0E-30jRG4KX"},"outputs":[],"source":["categorical_columns = df.select_dtypes(\n","    ['object', 'category']).columns.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzMh6xqtG4KX"},"outputs":[],"source":["interact(categorical_variable_distribution, col=categorical_columns);"]},{"cell_type":"markdown","metadata":{"id":"ThcVWC0pG4KX"},"source":["<hr>"]},{"cell_type":"markdown","metadata":{"id":"PNJca3DWG4KX"},"source":["So what can we summarize from our above steps?"]},{"cell_type":"markdown","metadata":{"id":"aZfQgTD7G4KX"},"source":["### <font color='#eb3483'> Data Dictionary <font color='#eb3483'>\n","\n","It is important to write down the description and datatypes of the variables.\n","(in our case it's pretty self explnatory but you may have done some major transformations and its good to try and keep track of them)  - especially for when sharing notebooks (think internships) or coming back to it months later.\n","\n","* id            --                     int64  \n","* host_id         --                categorical  \n","* host_since       --               date\n","* neighbourhood      --             categorical  \n","* city        --                    categorical  \n","* zipcode      --                   categorical  \n","* latitude      --                 float64  \n","* longitude     --                 float64  \n","* property_type       --            categorical  \n","* room_type           --            categorical  \n","* accommodates        --             int64  \n","* bathrooms           --           float64  \n","* bedrooms             --          float64  \n","* beds                 --          float64  \n","* bed_type             --           categorical  \n","* price                --            int64  \n","* cleaning_fee         --          float64  \n","* number_of_reviews    --            int64  \n","* review_scores_rating  --         float64  \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"o0JE9V2dG4KX"},"source":["### <font color='#eb3483'> Data processing steps </font>\n","- There are xxx duplicate rows (we have removed them)\n","- The variables `xxx, xxx, xxx and xxx` have missing values - what did we do with these?\n","- The categorical variable `xxx, xxx` has a dominant class (65% of xxx are xxx, etc)\n","- There are outliers in the variables `xxx and xxx` - what did we do with these?\n"]},{"cell_type":"markdown","metadata":{"id":"9ZtA1plbG4KY"},"source":["### <font color='#eb3483'> Variable Exploration Description <font color='#eb3483'>\n","(Distributions & Cardinality)  \n","Here we describe the possible entities(groupings) that we can break our dataset into, this will help us think of different ways to slice and group the dataset in further steps.\n","\n","- Use neighbourhood or zipcode (but what does Neighbourhood mean).   \n","- Most common zipcode is 8001 and ward is 115.\n","- City was almost all Cape Town, so not very informative for differentiation (ie low cardinality).\n","- Property_type - whole houses and apartments are the most common type.\n","- Room_type -> a lot of entire apartments and shared rooms.\n","- bed type -> predominently real beds. Not much value in this variable.\n","- Accommodates - > good range of sizes of properties.\n"]},{"cell_type":"markdown","metadata":{"id":"UqD7XOIxG4KY"},"source":["### <font color='#eb3483'>  Saving our data </font>\n","After each step it is important to save the dataset with a different name (so we dont modify the original)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l4j5NghKG4KY"},"outputs":[],"source":["df.to_csv(\"/content/drive/MyDrive/Class Materials/Week 1 DS Fundamentals/W1D4 EDA/Classwork/airbnb_processed.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"Hp5_fuqGG4KY"},"source":["<hr>\n","\n","### <font color='#eb3483'>An extra tit-bit: Pickling </font>\n"]},{"cell_type":"markdown","metadata":{"id":"eYFEI3lhG4KY"},"source":["We usually export our datasets to csv, because it is a format that is easily readable in pretty much any platform.\n","\n","However, CSV (`Comma Separated Values`) is a simple format, and when we export a dataframe to csv some of the information gets lost in translation, as categories will turn into text.\n","\n","One way to avoid this is to save the dataframe into a native python format `pickle`. Saving a dataframe as a pickle file has two main advantages. One is that reading the dataframe is much faster, because python can read the file from the hard drive as a dataframe directly (pandas doesnt have to read a text file and convert it into a dataframe). The second advantage is that we keep all of the original column dtypes.\n","\n","Pandas can read and write pickle files very easily (`read_pickle` and `to_pickle`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CsnyeVBiG4KY"},"outputs":[],"source":["df.to_pickle(\"/content/drive/MyDrive/Class Materials/Week 1 DS Fundamentals/W1D4 EDA/Classwork/airbnb_processed.pkl\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}