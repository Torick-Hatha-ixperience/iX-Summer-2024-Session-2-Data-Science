{"cells":[{"cell_type":"markdown","metadata":{"id":"TlOTGRGFb16x"},"source":["# <font color='#eb3483'> Decision Trees </font>\n","\n","One of the most popular algorithms for classification and regression are decision trees. One of the advantages of decision trees is their interpretability (you can see a tree and know exactly what it's doing)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y89S5MGjb16y"},"outputs":[],"source":["from IPython.display import Image\n","import pandas as pd\n","import numpy as np\n","import warnings\n","warnings.simplefilter(\"ignore\")\n","\n","import seaborn as sns\n","sns.set(rc={\"figure.figsize\":(6,6)})"]},{"cell_type":"markdown","metadata":{"id":"NN06VnDtb16z"},"source":["## <font color='#eb3483'> Load the data </font>\n","\n","We are going to use the [Titanic dataset](https://www.kaggle.com/c/titanic/data), and load it up from seaborn."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9a4qBUJb16z"},"outputs":[],"source":["passengers = sns.load_dataset(\"titanic\")\n","passengers.head()"]},{"cell_type":"markdown","metadata":{"id":"XeLHvBvYb16z"},"source":["## <font color='#eb3483'> Preprocessing </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yk8tcDNUb16z"},"outputs":[],"source":["#remove the alive column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wb18YYvXb16z"},"outputs":[],"source":["#check for missing values"]},{"cell_type":"markdown","metadata":{"id":"n-0Xdceyb16z"},"source":["We can drop the \"deck\" column, since 688 out of 891 instances are null, and we don't really have a way to fill in the missing values (unlike the \"age\" column, where we can use the average). We can also drop the \"embarked\" column since it has the same data as \"embark_town\". We can also drop the 2 rows with missing \"embark_town\" values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0VgDs50rb16z"},"outputs":[],"source":["# drop deck and embarked\n","dropped_columns = [\"deck\", \"embarked\"]\n","passengers = passengers.drop(dropped_columns, axis = 1)\n","\n","# fill in null age values with the average\n","passengers[\"age\"] = passengers[\"age\"].fillna(passengers[\"age\"].mean())\n","\n","# drop rows with null values\n","passengers = passengers.dropna()\n","\n","passengers.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lgoW40Fdb16z"},"outputs":[],"source":["#check the missing values again to check"]},{"cell_type":"markdown","metadata":{"id":"NVGy2scyb16z"},"source":["Decision trees can handle both categorical and numerical data. However, scikit-learn does not provide support for decision trees to handle categorical data! This is a long-standing issue... see https://stackoverflow.com/a/56857255 for more details on this.\n","\n","Because of this, using a OneHotEncoder is the only way to handle categorical data in decision trees in scikit-learn."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o36_TPBSb160"},"outputs":[],"source":["passengers = pd.get_dummies(passengers)\n","\n","passengers.head()"]},{"cell_type":"markdown","metadata":{"id":"XKCY-odzb160"},"source":["## <font color='#eb3483'> Classification with Decision Trees </font>"]},{"cell_type":"markdown","metadata":{"id":"ICFmHvzMb160"},"source":["In scikit-learn the tree based algorithms are in the `sklearn.tree` submodule.\n","\n","Scikit-learn tree implementation [uses an optimized version of CART](http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart) *(Classification and Regression Trees)*, that allows us to use the decission trees for both classification and regression."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2aSNQF_ib160"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score\n","from sklearn import metrics\n","\n","tree = DecisionTreeClassifier()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mlU_pJ0yb160"},"outputs":[],"source":["features = passengers.drop(columns=[\"survived\"]).columns\n","X = passengers[features]\n","y = passengers[\"survived\"]\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n","\n","tree.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P3jvlTASb160"},"outputs":[],"source":["#generate predictions based on your model and check the first 10 outputs of the array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q8ZfFLIKb160"},"outputs":[],"source":["#evaluate your model"]},{"cell_type":"markdown","metadata":{"id":"VFffLIgYb160"},"source":["One very good advantage that decision trees have is that they can be visualized, and we can explain why they take a certain decision  (we say trees have a high **explainability**). Scikit-learn trees can be visualized with a variety of tools; today we'll use `Grpahviz`, a visualisation library in Python. You can install it using `conda install python-graphviz`."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"7MD0o9Iwb160"},"outputs":[],"source":["import graphviz\n","import matplotlib.pyplot as plt\n","import pydotplus\n","import matplotlib.image as mpimg\n","import io\n","from sklearn.tree import export_graphviz\n","\n","def draw_tree(tree):\n","    dot_data = export_graphviz(tree,\n","                               out_file=None,\n","                               feature_names=features,\n","                               class_names=[\"survived\", \"died\"],\n","                               filled=True,\n","                               rounded=True,\n","                               special_characters=True,\n","                               proportion = True)\n","\n","    graph = graphviz.Source(dot_data)\n","    graph.format = \"png\"\n","    graph.render(\"tree\",view=True)\n","    plt.figure(figsize=(50,30))\n","    img = mpimg.imread(\"tree.png\")\n","    imgplot = plt.imshow(img)\n","\n","    plt.show()\n","\n","draw_tree(tree)\n"]},{"cell_type":"markdown","metadata":{"id":"y0mowyUpb160"},"source":["Another good functionality of decision trees is that they give us an indication of how important is each variable. A fitted tree can show us the variable importance with the attribute `feature_importances_`. The feature importances are calculated based on the information gained by each variable (that is, which variables split the classes better)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZkxd00Mb160"},"outputs":[],"source":["tree.feature_importances_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFqfNleVb160"},"outputs":[],"source":["dict(zip(\n","    features,\n","    tree.feature_importances_\n","))"]},{"cell_type":"markdown","metadata":{"id":"GP8GfM4Xb160"},"source":["For example, we see that on this tree the most important features in predicting if a passenger will survive are \"fare\", \"age\" and \"gender\"."]},{"cell_type":"markdown","metadata":{"id":"9OFEO8yGb160"},"source":["## <font color='#eb3483'> Max Depth </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-2H4wak3b160"},"outputs":[],"source":["DecisionTreeClassifier?"]},{"cell_type":"markdown","metadata":{"id":"-imELlchb161"},"source":["These are the most important hyperparameters for scikit-learn `DecisionTreeClassifier`:\n","\n","Here is a great article that goes over how to understand and potentially use these for tuning your model ...\n","https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680\n","\n","\n","* **criterion** : The partition criterion to use, we can use either `gini`, or `entropy`\n","\n","* **max_depth** (int>1) : The max depth the tree can achieve. We define as depth as the number of nodes an observation goes through (how many *questions* are asked).\n","\n","* **max_features** (int or float(percentage)):  The maximum number of potential partitions evaluated when we split a node.\n","\n","* **max_leaf_nodes** (int or None): Max number of leaves in the tree.\n","\n","* **min_impurity_decrease** (float) : The minimum information gain required in a node to split it (if no feature provides that minimum, the node becomes a leaf).\n","\n","* **class_weight** : For imbalanced classes, we can use `class_weight`, which is a dictionary with the shape `{class: weight}`, so sklearn takes the class weights into consideration. We can also use `class_weight=balanced` so sklearn creates the weights automatically based on class distribution.\n","\n","Decision trees are prone to overfitting, there are some hyperparameters that help us control this:\n","\n","* **min_samples_leaf** (int or float(percentage)) : Minimum number of observations on a node to consider the node a leaf. Default value is 1, this means that by default sklearn will create leaves with one observation (and this memorize the dataset).\n","\n","* **min_samples_split** (int or float(percentage)) : Minimum number of observations on a node to generate a partition. By default is 2, this means sklearn will split all nodes with 2 or more observations by default."]},{"cell_type":"markdown","metadata":{"id":"l2teUvurb161"},"source":["For example, we can create a simpler tree by setting the maximum depth"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-0IzHCbb161"},"outputs":[],"source":["simple_tree = DecisionTreeClassifier(max_depth=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XxjXjTdOb161"},"outputs":[],"source":["simple_tree.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pv2mrKaNb161"},"outputs":[],"source":["draw_tree(simple_tree)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KgcjvXHyb161"},"outputs":[],"source":["simple_predictions = simple_tree.predict(X_test)\n","print(metrics.confusion_matrix(y_test, simple_predictions))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qmi6_Fj4b161"},"outputs":[],"source":["print(metrics.accuracy_score(y_test, simple_predictions))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L-_78r3kb161"},"outputs":[],"source":["print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, simple_predictions))\n","print('Mean Squared Error:', metrics.mean_squared_error(y_test, simple_predictions))\n","print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, simple_predictions)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSmCUpt0b161"},"outputs":[],"source":["cross_val_score(simple_tree, X, y,\n","                scoring=\"roc_auc\",\n","                cv=3).mean()"]},{"cell_type":"markdown","metadata":{"id":"KfMJLp9rb162"},"source":["So we see this simple tree performs much better than the initial tree (that was overfitting), and it is also very simple to explain!"]},{"cell_type":"markdown","metadata":{"id":"dt87Tagrb162"},"source":["But how do we know what the optimal depth is? Well, this is a balance of practicality and \"hyperparameter tuning\". Let's test a number of depths."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Rb-uK7eb162"},"outputs":[],"source":["# let's test a range of depths from 2:10 using a for loop\n","\n","depths = np.arange(2,10) # define the depths\n","results = [] # create an empty data frame for our results\n","\n","for depth in depths:\n","    best_depth_tree = DecisionTreeClassifier(max_depth = depth) # creating an instance of a decision tree\n","    results.append(cross_val_score(best_depth_tree, X, y, scoring=\"roc_auc\", # getting the cv accuracy metric for the tree at each depth\n","                cv=3).mean())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1_4u-l8b162"},"outputs":[],"source":["test = pd.DataFrame({'depths':depths, 'mean_roc_auc':results})\n","test.sort_values(\"mean_roc_auc\", ascending=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}