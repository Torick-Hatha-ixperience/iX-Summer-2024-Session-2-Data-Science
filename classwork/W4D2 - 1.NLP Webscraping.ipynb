{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color='#eb3483'> Webscraping </font>\n",
        "\n",
        "Together, `BeautifulSoup` and `re` give us powerful tools for extracting information from webscraped data. Let's take a minute to see how this might work.  \n",
        "\n",
        "Suppose that we want to scrape the price information:\n",
        "\n",
        "https://webscraper.io/test-sites/e-commerce/allinone\n",
        "\n",
        "Take a minute to visit the webpage with your webrowser. If you are using Chrome, right click and select \"Inspect\". This will show you the underlying HTML code for this page.\n",
        "\n",
        "Let's webscrape! ... which really means reading the HTML code into Python. We use the `requests` package for this."
      ],
      "metadata": {
        "id": "5Yasg5cUXDjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install pandas\n",
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "qlRaXBo4ZVMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  <font color='#eb3483'> Approach using Regular Expressions </font>"
      ],
      "metadata": {
        "id": "ukBpvdTHXKui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "url = 'https://webscraper.io/test-sites/e-commerce/allinone'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')"
      ],
      "metadata": {
        "id": "yPvH84cyXNgi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all elements with the class 'price'\n",
        "prices = soup.find_all(class_='price')\n",
        "\n",
        "# Extract the text from each element\n",
        "for price in prices:\n",
        "    # Use a regular expression to extract the price in the format $XX.XX\n",
        "    price_text = price.get_text()\n",
        "    price_value = re.findall(r'\\$\\d+\\.\\d{2}', price_text)\n",
        "    if price_value:\n",
        "        print(price_value[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVAaNhfvXOPJ",
        "outputId": "123e88d3-92cd-4a27-b63e-eb38e66a35fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$306.99\n",
            "$899.99\n",
            "$494.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  <font color='#eb3483'> Extracting data from tables </font>"
      ],
      "metadata": {
        "id": "CrMVBP1ZX1nC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "dirnEgCFYRVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the Web Page\n",
        "url = \"https://www.worldometers.info/world-population/population-by-country/\"\n",
        "response = requests.get(url)\n",
        "html_content = response.content\n",
        "\n",
        "# Parse the HTML Content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')"
      ],
      "metadata": {
        "id": "r_WF64oWaY-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the Table\n",
        "table = soup.find('table', id='example2')  # Find the table with the specific id\n",
        "\n",
        "# Extract Table Headers\n",
        "headers = []\n",
        "for th in table.find_all('th'):\n",
        "    headers.append(th.text.strip())\n",
        "\n",
        "print(\"Headers:\", headers)"
      ],
      "metadata": {
        "id": "GNJCL6jxabX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Table Rows\n",
        "rows = []\n",
        "for tr in table.find_all('tr'):\n",
        "    cells = tr.find_all('td')\n",
        "    if len(cells) > 0:\n",
        "        row = [cell.text.strip() for cell in cells]\n",
        "        rows.append(row)\n",
        "\n",
        "# Print the first 5 rows to check\n",
        "for row in rows[:5]:\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "4dhFvK7Lahed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "df = pd.DataFrame(rows, columns=headers)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "qzL_TSxGZsMQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}