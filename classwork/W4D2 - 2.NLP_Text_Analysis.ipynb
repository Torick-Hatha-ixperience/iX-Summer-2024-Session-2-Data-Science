{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TVbId5gNHZ_"
      },
      "source": [
        "# <font color='#eb3483'> Natural Language Processing </font>\n",
        "\n",
        "In this notebook, we will be working with a large dataset of movie reviews from the **Internet Movie Database (IMDb)**. The dataset contains 50,000 movie reviews that have been labelled as positive or negative. Positive means that the movie got a rating of more than six stars, while negative means that it got a rating less than five stars.\n",
        "\n",
        "Our goal is to build a machine learning model to predict whether a reviewer will like or dislike a movie based on his/her written review.\n",
        "\n",
        "The dataset can be downloaded [here](//drive.google.com/file/d/1mLLHORSCShHdgQO_m1lTSnKC28EYWtnZ/view?usp=drive_link)\n",
        "\n"
      ],
      "id": "6TVbId5gNHZ_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pHBG4nlNHaB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re # for regular expressions\n",
        "import requests # to read the HTML at a URL into Python\n",
        "from bs4 import BeautifulSoup # to extract text from HTML\n",
        "import nltk # natural language toolkit for stop words, stemming and lemmatization (and more!)"
      ],
      "id": "2pHBG4nlNHaB"
    },
    {
      "cell_type": "code",
      "source": [
        "#Import from Data Folder in Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GbJofvCFNVXw"
      },
      "id": "GbJofvCFNVXw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Data Science | Abroad | S1 | Claire/Class Materials/Week 3-6 Special Topics/W4D2 NLP/Classwork/movies.csv\"\n",
        "df = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "TY7Tia12NXtg"
      },
      "id": "TY7Tia12NXtg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hs34n47YNHaB"
      },
      "outputs": [],
      "source": [
        "#df = pd.read_csv('data/movies.csv')\n",
        "df"
      ],
      "id": "hs34n47YNHaB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of 0s and 1s in 'sentiment'\n",
        "count_values = df['sentiment'].value_counts()\n",
        "\n",
        "print(count_values)"
      ],
      "metadata": {
        "id": "1dHFnOPaITW_"
      },
      "id": "1dHFnOPaITW_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9xScSb_NHaC"
      },
      "source": [
        "## <font color='#eb3483'> Before We Start: List Comprehension in Python </font>\n",
        "\n",
        "Before we begin our intro to natural language processing, we need to discuss **list comprehension** in Python: an easy way to loop over and select/transform items in a list.\n",
        "\n",
        "As an example, suppose you have a list:\n",
        "\n",
        "```python\n",
        "mylist = [1,2,3,4,5]\n",
        "```\n",
        "\n",
        "and you want to return a new list that contains the squared values of each element. How would you do this?\n",
        "\n",
        "Well, you could use a ```for``` loop:"
      ],
      "id": "x9xScSb_NHaC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKStcJkANHaC"
      },
      "outputs": [],
      "source": [
        "mylist = [1,2,3,4,5]\n",
        "newlist = []\n",
        "\n",
        "for i in range(len(mylist)):\n",
        "    newlist.append(mylist[i]**2)\n",
        "\n",
        "newlist"
      ],
      "id": "HKStcJkANHaC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x66NUXJxNHaC"
      },
      "source": [
        "A neater way to do this is to use **list comprehension**, which essentially includes the ``for`` loop within the list:"
      ],
      "id": "x66NUXJxNHaC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2zjK8otNHaC"
      },
      "outputs": [],
      "source": [
        "[x**2 for x in mylist]"
      ],
      "id": "l2zjK8otNHaC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBVXl_0ONHaC"
      },
      "source": [
        "We can even include an `if` statement in our list comprehension to select only a subset of the items:"
      ],
      "id": "lBVXl_0ONHaC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgjz-qu0NHaC"
      },
      "outputs": [],
      "source": [
        "[x**2 for x in mylist if x < 4]"
      ],
      "id": "bgjz-qu0NHaC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD61UL1QNHaD"
      },
      "source": [
        "## <font color='#eb3483'> Text Cleaning and Preprocessing </font>\n",
        "\n",
        "As you'll soon see, converting textual data into a format that can be used for machine learning can take a lot of time and effort. Often the techniques that we use are problem specific. Here, we will explore some of these approaches as they apply to our movies dataset. If you ever find yourself analysing text data, you'll probably need to adapt these methods to suit your needs."
      ],
      "id": "jD61UL1QNHaD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color='#eb3483'> 1. HTML </font>\n",
        "\n",
        "Have a look at the first and fourth reviews (indexed as 0 and 3):"
      ],
      "metadata": {
        "id": "otNZ0JshQNbL"
      },
      "id": "otNZ0JshQNbL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hea7dA0UNHaD"
      },
      "outputs": [],
      "source": [
        "df.review.iloc[0]"
      ],
      "id": "Hea7dA0UNHaD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVngMlQJNHaD"
      },
      "outputs": [],
      "source": [
        "df.review.iloc[3]"
      ],
      "id": "sVngMlQJNHaD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zbAST5fNHaD"
      },
      "source": [
        "You'll notice that they contain expressions like ```<br />```. These are HTML tags - the ``<br>`` tag is used to display a line **br**eak in HTML code. The details are not important, but we probably don't want to keep these in the text.\n",
        "\n",
        "Let's remove HTML tags using the ``BeautifulSoup`` function from ``bs4``, an extremely useful library for extracting text from HTML and XML files."
      ],
      "id": "2zbAST5fNHaD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4-pGoNtNHaD"
      },
      "outputs": [],
      "source": [
        "BeautifulSoup(df.review.iloc[3], 'html.parser').get_text()"
      ],
      "id": "H4-pGoNtNHaD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD5JEAq-NHaD"
      },
      "source": [
        "Ah, that looks better! Let's apply this function to all the reviews in our dataset."
      ],
      "id": "KD5JEAq-NHaD"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean HTML tags\n",
        "def clean_html(text):\n",
        "    return BeautifulSoup(text, 'html.parser').get_text()\n",
        "\n",
        "# Apply the function to the entire column\n",
        "df['review'] = df['review'].apply(clean_html)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "9DJqDu4jJOSd"
      },
      "id": "9DJqDu4jJOSd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use regular expressions library"
      ],
      "metadata": {
        "id": "z-1aVCM1J1im"
      },
      "id": "z-1aVCM1J1im"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQP7HlT5NHaD"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_html(x):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, '', x)\n",
        "    return cleantext\n",
        "\n",
        "df['review'] = df['review'].apply(remove_html)"
      ],
      "id": "LQP7HlT5NHaD"
    },
    {
      "cell_type": "code",
      "source": [
        "df.review.iloc[44]"
      ],
      "metadata": {
        "id": "IPosHXfCO-dF"
      },
      "id": "IPosHXfCO-dF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mciYaBfnNHaD"
      },
      "source": [
        "###  <font color='#eb3483'> 2. Regular Expressions </font>\n",
        "\n",
        "If you take a look at a few of the reviews, you'll notice that people write differently. Some write \"I\" with a capital letter, while others don't worry about capitalization and simply write \"i\". Some write \"cant\" while others correctly write \"can't\" with the apostrophe. Some put a space after a full stop, others don't. We don't want these arbitrary differences affecting out analyses, so let's try and standardize the text. For example, let's convect all letters to lower case:"
      ],
      "id": "mciYaBfnNHaD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jab7FJT4NHaD"
      },
      "outputs": [],
      "source": [
        "myreview = df.review.iloc[3]\n",
        "myreview = myreview.lower()\n",
        "myreview"
      ],
      "id": "jab7FJT4NHaD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2jQLSHfNHaE"
      },
      "source": [
        "Now let's find and replace all the commas with a space. You're probably used to doing this on your computer in MS Word - you just hit ``ctrl/cmd + F``, type in the letter/word you want to find, and then replace it. Turns out you can also do this programmatically using the ``sub`` (substitute) function from the `re` module:"
      ],
      "id": "D2jQLSHfNHaE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT3LJ63PNHaE"
      },
      "outputs": [],
      "source": [
        "re.sub(\",\", \" \", myreview)"
      ],
      "id": "DT3LJ63PNHaE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GldMtaUmNHaE"
      },
      "source": [
        "``re.sub`` is actually way more powerful than this. As its first argument, it accepts a **regular expression** or **regex** which is basically a *search pattern*. We can search for **any** pattern we like. For example, we could search a text string for all phone numbers that have the format +XX-XXX-XXX-XXXX, where X is a number. A regular expression is a way to express this pattern programmatically.\n",
        "\n",
        "We are not going to have time to cover all the possible ways to express patterns, so we'll just consider the ones that are useful for this dataset. For more information in regular expressions, look at the [```re``` help page](https://docs.python.org/3/library/re.html) or Google around."
      ],
      "id": "GldMtaUmNHaE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5gpvvhgNHaE"
      },
      "source": [
        "We previously replaced all commas with a space. Let's also replace full stops, underscores, question marks and exclamation marks. The regex for this is ``[,._?!]`` which translates into \"find all instances of ``,`` or ``.``\". Note that the square brackets are simply used to enclose the different symbols that we want to find. They are not included in the search."
      ],
      "id": "B5gpvvhgNHaE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPOXBtKoNHaE"
      },
      "outputs": [],
      "source": [
        "re.sub(\"[,._?!]\", \" \", myreview)"
      ],
      "id": "vPOXBtKoNHaE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xnIaDghNHaE"
      },
      "source": [
        "Let's also get rid of the numbers. Here the regex ``[0-9]`` reads \"find any number from 0 to 9\"."
      ],
      "id": "8xnIaDghNHaE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja3wEbC5NHaE"
      },
      "outputs": [],
      "source": [
        "re.sub(\"[0-9]\", \" \", myreview)"
      ],
      "id": "ja3wEbC5NHaE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl7c2hFNNHaE"
      },
      "source": [
        "Let's combine are two regexs to simulatanously substitute commas, fullstops, exclamation and question marks, AND all numbers:"
      ],
      "id": "kl7c2hFNNHaE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC5wPcr5NHaE"
      },
      "outputs": [],
      "source": [
        "re.sub(\"[,._?!0-9]\", \" \", myreview)"
      ],
      "id": "kC5wPcr5NHaE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEnHfKXNNHaE"
      },
      "source": [
        "We also probably want to get rid of the dashes in \"sex-life\" and \"police-officers\" so that these are each represented by two separate words across all reviewers. Can we just add a \"-\" into our regex?\n",
        "\n",
        "Well, no... because a dash is already being used to specify the range of numbers 0-9. Because it already has a special use in a regex, if we want to include it, we must write ``\\-`` in the regex:\n",
        "\n"
      ],
      "id": "zEnHfKXNNHaE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaJeTzQxNHaE"
      },
      "outputs": [],
      "source": [
        "re.sub(\"[,._?!0-9\\-]\", \" \", myreview)"
      ],
      "id": "uaJeTzQxNHaE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMXpjcKANHaE"
      },
      "source": [
        "Similarly, the following *special* symbols also need to be preceded by a slash \"\\\\\":\n",
        "\n",
        "( ) [ ] & | \" / \\"
      ],
      "id": "DMXpjcKANHaE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x80oHGyPNHaF"
      },
      "outputs": [],
      "source": [
        "re.sub(\"[.,_\\-!?\\(\\)\\/0-9]\", \" \", myreview)"
      ],
      "id": "x80oHGyPNHaF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35Q8ugcsNHaF"
      },
      "source": [
        "You may have noticed that we can end up with a lot of space between some words. A space is itself a character (we just can't see it). We can match a single space with the regex `[ ]`, but what if we want to match 1 or more spaces, so that we can, for instance, replaces three consecutive spaces with a single space? Easy peazy! We just use `[ ]+` as our regex:"
      ],
      "id": "35Q8ugcsNHaF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vjy2AIuNHaF"
      },
      "outputs": [],
      "source": [
        "myreview = re.sub(\"[.,_\\-!?\\(\\)\\/0-9]\", \" \", myreview)\n",
        "myreview = re.sub(\"[ ]+\", \" \", myreview)\n",
        "myreview"
      ],
      "id": "9vjy2AIuNHaF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQDRZUllNHaF"
      },
      "source": [
        "No more multiple spaces! Finally, let's remove the apostrophes from all contractions (e.g. can't, don't, etc) and possesive words (reviewer's --> reviewers), so that this is handled consistently across reviewers. Instead of replacing it with a space, we will replace it with nothing (that is, ``\"\"``) so we effectly remove it:"
      ],
      "id": "bQDRZUllNHaF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3VyuXp9NHaF"
      },
      "outputs": [],
      "source": [
        "myreview = re.sub(\"'\", \"\", myreview)\n",
        "myreview"
      ],
      "id": "d3VyuXp9NHaF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E806uNm8NHaF"
      },
      "source": [
        "That looks good! Let's put everything we've done into a single function and apply it to all reviews:"
      ],
      "id": "E806uNm8NHaF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqob29EyNHaF"
      },
      "outputs": [],
      "source": [
        "def reformat_string(x):\n",
        "    x = x.lower() # change to a lower case\n",
        "    x = re.sub('[.,_\\-!?\\(\\)\\/\\\"\\&0-9]', \" \", x) # remove certain characters\n",
        "    x = re.sub(\"[ ]+\", \" \", x) # replace multiple spaces with a single one\n",
        "    x = re.sub(\"'\", \"\", x) # remove apostrophes\n",
        "    return x\n",
        "\n",
        "df['review'] = df.review.apply(reformat_string)"
      ],
      "id": "aqob29EyNHaF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUUaDoc_NHaF"
      },
      "outputs": [],
      "source": [
        "df.review.iloc[0]"
      ],
      "id": "JUUaDoc_NHaF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er9mBsiVNHaF"
      },
      "outputs": [],
      "source": [
        "df.review.iloc[1]"
      ],
      "id": "er9mBsiVNHaF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fuFV8tXNHaM"
      },
      "source": [
        "### <font color='#eb3483'> 3. Stop Words </font>\n",
        "\n",
        "Let's go back to our movies dataset and have a look at the first review, for example:"
      ],
      "id": "4fuFV8tXNHaM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk9qzKzKNHaM"
      },
      "outputs": [],
      "source": [
        "df.review.iloc[0]"
      ],
      "id": "sk9qzKzKNHaM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O99wYjPgNHaM"
      },
      "source": [
        "Many of the words in this sentence are unlikely to be helpful for predicting the reviewer's sentiment. For example, \"with\", \"of\", \"the\", \"this\" and so on. Such words are referred to as **stop words**, and we would usually like to remove them from the sentence.\n",
        "\n",
        "The ```nltk``` (**n**atural **l**anguage **t**ool**k**it) library contains a set of 179 English stop words. We can use this list to omit stop words from our reviews:"
      ],
      "id": "O99wYjPgNHaM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXV5cnbnNHaM"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords') # need to run this the first time only\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "print('List contains', len(stop), 'stopwords')\n",
        "stop"
      ],
      "id": "LXV5cnbnNHaM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v1OV5bPNHaM"
      },
      "source": [
        "Note that the stop words list contains contractions such as \"you're\" and \"you've\" that include an apostophe. Since we removed the apostrophes in our reviews, we will begin by removing them in the stop word list, and then exclude all the words in the stop word list from our reviews."
      ],
      "id": "9v1OV5bPNHaM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSUrVPeYNHaM"
      },
      "outputs": [],
      "source": [
        "stop = [re.sub(\"'\", \"\", w) for w in stop] # list comprehension\n",
        "stop"
      ],
      "id": "SSUrVPeYNHaM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCMwwGYUNHaM"
      },
      "outputs": [],
      "source": [
        "df.review.iloc[0]"
      ],
      "id": "gCMwwGYUNHaM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvO-HUbZNHaN"
      },
      "outputs": [],
      "source": [
        "# Split the string in the first element into a list of words.\n",
        "# Iterate over each word w in the list of words and include w in the resulting list only if it is not in the stop list.\n",
        "# Join the words back together with a space seperating each word\n",
        "' '.join([w for w in df.review.iloc[0].split() if w not in stop])"
      ],
      "id": "tvO-HUbZNHaN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woVMC50zNHaN"
      },
      "source": [
        "Now we're ready to put this into a general function that we can apply to all reviews in our dataset:"
      ],
      "id": "woVMC50zNHaN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzWzbcCnNHaN"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(x):\n",
        "    return ' '.join([w for w in x.split() if w not in stop])\n",
        "\n",
        "df['review'] = df.review.apply(remove_stopwords)\n",
        "df.head()"
      ],
      "id": "dzWzbcCnNHaN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvYU-Sx5NHaN"
      },
      "source": [
        "### <font color='#eb3483'> 5. Stemming & Lemmatization </font>\n",
        "\n",
        "Many words with the same meaning can be written in slighly different ways depending, for example, on tense (past, present and future tense) and plurality (singular vs plural). For example, \"run\", \"ran\", \"runs\" and \"running\" all refer to the same concept of \"running\" and we would therefore like to represent all of these different words as a single feature in our model for predicting sentiment. We can use stemming or lemmatization to achieve this.\n",
        "\n",
        "**Stemming** is the process of transforming a word into its root form to allow us to map related words to the same stem. There are many different stemming algorithms; we will use the first such algo developed by Martin Porter in 1979 and thus known as Porter stemming:"
      ],
      "id": "xvYU-Sx5NHaN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5YoqKUANHaN"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "id": "E5YoqKUANHaN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zDQ1v-6NHaN"
      },
      "outputs": [],
      "source": [
        "def stemming(x):\n",
        "    words = [stemmer.stem(w) for w in x.split()] # do stemming\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['review'] = df.review.apply(stemming) # this may take a few minutes if the dataset is large\n",
        "df.head()"
      ],
      "id": "8zDQ1v-6NHaN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgveI8m6NHaN"
      },
      "source": [
        "Sometimes, stemming produces non-real words. This is usually not a problem. However, if we want gramatically correct words, we can use a similar process called **lemmatization** that attempts to identify the canonical form of a word:"
      ],
      "id": "XgveI8m6NHaN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7PlVg_nNHaN"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet') # run this the first time you use it\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to lemmatize each word in a string\n",
        "def lemmatize_text(text):\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "# Apply the function to the 'text' column\n",
        "df['review'] = df['review'].apply(lemmatize_text)\n",
        "\n",
        "df.head()"
      ],
      "id": "j7PlVg_nNHaN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umIqM7zBNHaO"
      },
      "source": [
        "## <font color='#eb3483'> Formatting the Data for Machine Learning </font>\n",
        "\n",
        "Now that the reviews have been cleaned, we need to convert them into features that can be used in an ML algo.\n",
        "\n",
        "In a **bag-of-words** model, the frequency of each word in a sentence is regarded as a separate feature, *ignoring its context or adjacent words within a sentence*. In our movies dataset, the idea is that certain words would be more common in negative reviews, while other words would occur more frequently in positive reviews.\n",
        "\n",
        "Since feature engineering should always be performed on the training set and **not** on the test data, we begin by dividing our movies data into train and test sets, and make all of our decisions on the training data. In this dataset, I have already ordered the rows, so that the first 25,000 rows are the training set and the last 25,000 rows are the test set:"
      ],
      "id": "umIqM7zBNHaO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzfXIgP8NHaO"
      },
      "outputs": [],
      "source": [
        "train = df[:25000]\n",
        "test = df[25000:]"
      ],
      "id": "GzfXIgP8NHaO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRPiGX9vNHaO"
      },
      "outputs": [],
      "source": [
        "train.sentiment.value_counts()"
      ],
      "id": "zRPiGX9vNHaO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYDxiIHnNHaO"
      },
      "outputs": [],
      "source": [
        "test.sentiment.value_counts()"
      ],
      "id": "wYDxiIHnNHaO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZGscJNaNHaO"
      },
      "source": [
        "### <font color='#eb3483'> 1. Tokenization </font>\n",
        "\n",
        "The process of splitting the sentences into words is referred to as **tokenization**. In this case, a word is referred to as a **token**. More generally though, a token could be a word pair, triplet or an even longer string of adjacent words.\n",
        "\n",
        "To contruct our features, we need to get the counts of **all the words/tokens** across **all the reviews** in our training data (25,000 reviews). This is going to be a very large number of words! We refer to this as the **vocabularly**.\n",
        "\n"
      ],
      "id": "2ZGscJNaNHaO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5XgstpNNHaO"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "count_matrix = count_vectorizer.fit_transform(train.review.iloc[:5]) # apply just to the first 5 reviews"
      ],
      "id": "S5XgstpNNHaO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyQK59RqNHaO"
      },
      "outputs": [],
      "source": [
        "count_matrix"
      ],
      "id": "kyQK59RqNHaO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTtIRH-bNHaO"
      },
      "source": [
        "The output of the ```CountVectorizer``` is a sparse matrix. Most of the counts are zero, so Python uses a memory efficient format to store these data (it only needs to store the non-zero entries in memory). If we want to \"see\" the matrix, we can use the ```.toarray()``` function:"
      ],
      "id": "mTtIRH-bNHaO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Osknp6gGNHaO"
      },
      "outputs": [],
      "source": [
        "count_matrix.toarray()"
      ],
      "id": "Osknp6gGNHaO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1YrnbPzNHaO"
      },
      "source": [
        "Because we only apply the ```CountVectorizer``` to the first 5 reviews, this matrix is still small. It has 533 columns (a vocab of size 533). But if we were to apply it to the full dataset, the number of columns (words) would be more than 50,000, with 25,000 rows! So it makes sense to store this as efficiently as possible. You can extract the vobabularly from the fitted ```CountVectorizer``` object:"
      ],
      "id": "n1YrnbPzNHaO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7N1kf7mfNHaO"
      },
      "outputs": [],
      "source": [
        "count_vectorizer.vocabulary_"
      ],
      "id": "7N1kf7mfNHaO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqCcWUyiNHaO"
      },
      "source": [
        "Note that this is a dictionary that maps each word to a column in our ``count_matrix`` array."
      ],
      "id": "ZqCcWUyiNHaO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ10uZ2mNHaO"
      },
      "outputs": [],
      "source": [
        "count_vectorizer.vocabulary_. keys() # just the vocabulary"
      ],
      "id": "OQ10uZ2mNHaO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQFyCqaDNHaP"
      },
      "outputs": [],
      "source": [
        "# a neater version of the count_matrix:\n",
        "pd.DataFrame(count_matrix.toarray(), columns=pd.Series(count_vectorizer.vocabulary_).sort_values().index)"
      ],
      "id": "EQFyCqaDNHaP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma-vyUc_NHaP"
      },
      "source": [
        "As mentioned earlier, we could have considered word pairs as our tokens. This is called a **bigram**. This may be useful to deal with **negation** e.g. the bigram \"not good\" actually has the opposite meaning to unigram \"good\".  More generally, a token comprising $n$ words is referred to as an **$n$-gram**. Here's how we would implement bigram tokenization with ```CountVectorizer```:"
      ],
      "id": "Ma-vyUc_NHaP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3imYoGRRNHaP"
      },
      "outputs": [],
      "source": [
        "bigram_vectorizer = CountVectorizer(ngram_range = (2,2))\n",
        "bigram_matrix = bigram_vectorizer.fit_transform(train.review.iloc[:5])\n",
        "bigram_vectorizer.vocabulary_.keys() # a dictionary mapping words to columns"
      ],
      "id": "3imYoGRRNHaP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_yiUeZNNHaP"
      },
      "source": [
        "### <font color='#eb3483'> 2. Term Frequency-Inverse Document Frequency (TF-IDF) </font>\n",
        "\n",
        "The tokenization process can often produce very many features (i.e. a LARGE vocabularly). Part of the feature engineering process is to try and reduce less very large feature space down, which amounts to selected a subset of the vocabularly that we think is likely to be most relevant for predicting the outcome variable (sentiment, in our example).\n",
        "\n",
        "The frequency of a term (word) can certainly help here. As we mentioned earlier, if a word tends to occur more frequently among positive reviews than negative ones, then it is likely to be a good predictor. However, terms/words that occur frequently across all documents are not very informative e.g. words like \"with\" and \"is\". We therefore want to keep words that occur often within a document, but not often in all documents. The term frequency-inverse document frequency (TF-IDF) attempts to measure this. Higher values indicate that a word is more relevant, and can be used for feature selection."
      ],
      "id": "r_yiUeZNNHaP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq1O-iQzNHaP"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(train.review)\n",
        "\n",
        "print('The vocularly has', len(tfidf_vectorizer.vocabulary_.keys()), 'words')"
      ],
      "id": "aq1O-iQzNHaP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTuhyoxQNHaP"
      },
      "outputs": [],
      "source": [
        "tfidx_df = pd.DataFrame(tfidf_matrix.toarray(), columns=pd.Series(tfidf_vectorizer.vocabulary_).sort_values().index)\n",
        "tfidx_df.head()"
      ],
      "id": "WTuhyoxQNHaP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTPBX9oYNHaP"
      },
      "outputs": [],
      "source": [
        "word_relevance = tfidx_df.sum().sort_values(ascending=False)\n",
        "word_relevance.head(20)"
      ],
      "id": "LTPBX9oYNHaP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88V3IvPdNHaP"
      },
      "outputs": [],
      "source": [
        "word_relevance.tail(20)"
      ],
      "id": "88V3IvPdNHaP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1o7WMi6NHaP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "graph = sns.lineplot(x=np.arange(5000),y=word_relevance.iloc[:5000])\n",
        "graph.axvline(500, c='r')\n"
      ],
      "id": "K1o7WMi6NHaP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YFFbEhHNHaP"
      },
      "source": [
        "The resulting plot will have a red vertical line at the x-coordinate 500, which can be used to highlight a specific point or threshold in the data.\n",
        "\n",
        "Let's keep just the top 500 words/features for model building:"
      ],
      "id": "6YFFbEhHNHaP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBrXnfbdNHaP"
      },
      "outputs": [],
      "source": [
        "vocab = word_relevance[:500].index\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(vocabulary=vocab)\n",
        "X_train = tfidf_vect.fit_transform(train.review)\n",
        "X_test  = tfidf_vect.transform(test.review)"
      ],
      "id": "DBrXnfbdNHaP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLvqclL3NHaP"
      },
      "outputs": [],
      "source": [
        "print('Training data shape:', X_train.shape)\n",
        "print('Test data shape:', X_test.shape)"
      ],
      "id": "zLvqclL3NHaP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-JaRp_wNHaQ"
      },
      "source": [
        "## <font color='#eb3483'> Model Building </font>\n",
        "\n",
        "Now that we have a set of features and a discrete outcome variable, we can go ahead and train whichever classifier we choose... or better still, try a few different classification algos and choose the best one using the test data!\n",
        "\n",
        "As an example, let's train an out-of-the-box random forest:"
      ],
      "id": "2-JaRp_wNHaQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep-adAOcNHaQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X=X_train, y=train.sentiment) # this may take a few minutes"
      ],
      "id": "ep-adAOcNHaQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH7oXiIsNHaQ"
      },
      "outputs": [],
      "source": [
        "varimp = pd.Series(rf.feature_importances_, index=vocab).sort_values(ascending=False)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.barplot(x=varimp[:20], y=varimp.index[:20])"
      ],
      "id": "DH7oXiIsNHaQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NfpuAiyNHaQ"
      },
      "outputs": [],
      "source": [
        "y_pred = rf.predict(X_test)"
      ],
      "id": "7NfpuAiyNHaQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2x1AynSNHaQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, RocCurveDisplay\n",
        "print('Confusion matrix:\\n', confusion_matrix(test.sentiment, y_pred), '\\n')\n",
        "print('Test accuracy:', accuracy_score(test.sentiment, y_pred))\n",
        "RocCurveDisplay.from_estimator(rf, X_test, test.sentiment)"
      ],
      "id": "w2x1AynSNHaQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE9VcHuxNHaQ"
      },
      "source": [
        "## <font color='#eb3483'> Pre-Trained Sentiment Classifier </font>\n",
        "\n",
        "In the work above, we trained our own sentiment classifier. This is often useful to identify sentiment within a specific fields, where certain worlds have special meanings. For example, if you were to develop a sentiment classifier for financial news headlines, you would want the word \"bull\" to have positive sentiment and \"bear\" to have a negative sentiment (prices rise in \"bull markets\" and plummet in \"bear markets\"). In most other contexts, the sentiment around bulls and bears would probably be different.\n",
        "\n",
        "For some domains, pre-trained models are available. Like our model above, these models typically map words to sentiment scores (called a **lexicon**) that are then combined for a given sentence. Other folk have trained the model on a specific dataset (just like we did above) and made the model available to us.\n",
        "\n",
        "**For example, the VADER VADER (Valence Aware Dictionary and sEntiment Reasoner) lexicon is a sentiment analysis tool that is particularly effective for analyzing social media text.**"
      ],
      "id": "pE9VcHuxNHaQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4U0ygioNHaQ"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()"
      ],
      "id": "p4U0ygioNHaQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sia.polarity** calls the polarity_scores method from the VADER sentiment analysis tool (sia) and returns a dictionary of sentiment scores for the input text x.\n",
        "\n",
        "The dictionary contains four keys: neg, neu, pos, and compound. The compound score is a normalized, weighted composite score that summarizes the overall sentiment of the text."
      ],
      "metadata": {
        "id": "eIue-7GW2Ijy"
      },
      "id": "eIue-7GW2Ijy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atQNOJn6NHaQ"
      },
      "outputs": [],
      "source": [
        "#This calls the polarity_scores method from the VADER sentiment analysis tool (sia)\n",
        "sia.polarity_scores(\"love\")"
      ],
      "id": "atQNOJn6NHaQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhmNsP0dNHaQ"
      },
      "outputs": [],
      "source": [
        "sia.polarity_scores(\"hate\")"
      ],
      "id": "WhmNsP0dNHaQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8Aq61hGNHaQ"
      },
      "outputs": [],
      "source": [
        "sia.polarity_scores(\"bull\")"
      ],
      "id": "-8Aq61hGNHaQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVPBZ9UyNHaQ"
      },
      "outputs": [],
      "source": [
        "sia.polarity_scores(\"I love waffles and ice-cream\")"
      ],
      "id": "AVPBZ9UyNHaQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he3uXFBaNHaQ"
      },
      "source": [
        "Let's see how well the VADER lexicon captures the sentiment in our movie reviews:"
      ],
      "id": "he3uXFBaNHaQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5ekxYp8NHaQ"
      },
      "outputs": [],
      "source": [
        "vader_sentiment = [sia.polarity_scores(x)['compound'] for x in df.review] #This is a list comprehension that iterates over each review in the df.review column, calculates the compound sentiment score for each review, and stores these scores in the list vader_sentiment.\n",
        "pd.crosstab(np.array(vader_sentiment) > 0, df.sentiment)"
      ],
      "id": "U5ekxYp8NHaQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2eDMYvfNHaR"
      },
      "source": [
        "We see that when a movie review has positive sentiment, the VADER lexicon agrees 20731/(20731+4269) = 83% of the time.\n",
        "\n",
        "But when the reviewer's sentiment is actually negative, VADER only agrees 11811/(11811+13189) = 47% of the time.\n",
        "\n",
        "Perhaps there are differences between the way people express sentiment in social media compared to in movie reviews?"
      ],
      "id": "I2eDMYvfNHaR"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}